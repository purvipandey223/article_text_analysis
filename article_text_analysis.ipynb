{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b194f915-0688-47b2-be07-80e1990f84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "import openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "import nltk\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664a9ebf-4a1b-4048-be84-6f93f8f70074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ppurv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b115d50b-ec3e-48c6-a1c6-7179c22e9db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/',\n",
       "  'https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/',\n",
       "  'https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/',\n",
       "  'https://insights.blackcoffer.com/the-rise-of-the-ott-platform-and-its-impact-on-the-entertainment-industry-by-2040/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cyber-crime-and-its-effects/',\n",
       "  'https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040/'],\n",
       " 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Input.xlsx')\n",
    "urls = list(df['URL'])\n",
    "urls[:10] , len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33b683-720b-4eb3-8000-169155a258b9",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f27c873e-b017-4d43-82b0-937012dea115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rise of OTT platform and its impact on entertainment industry by the year 2030\\n The advancement in technology and proliferation of the Internet has led to the wide exposure of entertainment platforms to a massive audience including teenagers, adults, and senile. In this research paper, we study the impact of Over-the-Top media on society as a whole especially after the pandemic when there was a sudden shift in remote work culture, educational practices being held online, families spending valuab'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_web_data(url):\n",
    "    class_ = ['td-post-content tagdiv-type', 'tdb-block-inner td-fix-index']\n",
    "    doc = requests.get(url)\n",
    "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
    "    title = soup.find(\"h1\")\n",
    "    article = soup.find_all('div',{\"class\": class_[0]})\n",
    "    if article :\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res+=tag.text.strip()\n",
    "    else:\n",
    "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res+=tag.text.strip()\n",
    "    try:\n",
    "        start = res.index(\"Introduction\")\n",
    "        stop = res.index(\"Blackcoffer Insights\")\n",
    "    except:\n",
    "        start = 0\n",
    "        stop = -1\n",
    "    return title.text + \"\\n\" +res[start:stop]\n",
    "\n",
    "fetch_web_data(random.choice(urls))[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f273b3a-826e-45bb-85d4-6bb738d7adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " 'ARIARY']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words():\n",
    "    StopWords_notNames = []\n",
    "    for file in os.listdir(\"StopWords/StopWords\"):\n",
    "        if file != \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            res = []\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
    "            if res != []:\n",
    "                StopWords_notNames.extend(res)\n",
    "\n",
    "    StopWords_Names = []\n",
    "    for file in os.listdir(\"StopWords/StopWords\"):\n",
    "        if file == \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
    "                    if res != None:\n",
    "                        StopWords_Names.append(res[0])\n",
    "\n",
    "    stop_words = []\n",
    "    for file in os.listdir(\"StopWords/StopWords\"):\n",
    "        corpus = open(f\"StopWords/StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
    "        res = []\n",
    "        for txt in corpus:\n",
    "            if \"|\" in txt:\n",
    "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
    "                res.append(txt.strip())\n",
    "        if res != []:\n",
    "            stop_words.extend(res)\n",
    "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
    "\n",
    "    stop_words.extend(StopWords_notNames)\n",
    "    stop_words.extend(StopWords_Names)\n",
    "    return stop_words\n",
    "\n",
    "get_stop_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e19f0f9e-dd68-443a-b5a2-1732ff66e424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rise Cyber Crime Effects Cybercrime discussed problem twenty first century. usage cell phones internet increasing dramatically world generating questions consumers security privacy. this users understand cybercrime security. Cybercrime defined organised criminal conduct carried attackers online. Cybercrime numerous forms fraud computer viruses cyberstalking others. Due these businesses government organisations spending maintaining employing professionals cybercrime. Cyber security keywords Artif'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_stop_words(text, personalwords = True):\n",
    "    stop_words = get_stop_words()\n",
    "    if personalwords == True:\n",
    "        stop_words.extend(nltk_sw.words('english'))\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    cleaned_text = ' '.join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
    "    return cleaned_text\n",
    "\n",
    "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e53f95-922f-4edd-a75d-76f3cf45b9fe",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42ed22e7-a6bf-40ea-b05f-c38faabe403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 6, 0.6578947368421053, 0.7241379310344828)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(text):\n",
    "    def get_subjectivity_score(text):\n",
    "        num_words = len(text.split())\n",
    "        unique_words = len(set(text.split()))\n",
    "        subjectivity_score = unique_words / num_words\n",
    "        return subjectivity_score\n",
    "\n",
    "    def get_polarity_score(text):\n",
    "        positive_words = (\n",
    "            open(\"MasterDictionary/MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "        negative_words = (\n",
    "            open(\"MasterDictionary/MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word.lower() in positive_words:\n",
    "                positive_count += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_count += 1\n",
    "\n",
    "        polarity_score = (positive_count - negative_count) / (\n",
    "            positive_count + negative_count + 1\n",
    "        )\n",
    "        return polarity_score, positive_count, negative_count\n",
    "\n",
    "    subjectivity_score = get_subjectivity_score(text)\n",
    "    polarity_score, positive_count, negative_count = get_polarity_score(text)\n",
    "\n",
    "    return positive_count, negative_count, polarity_score, subjectivity_score\n",
    "\n",
    "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418701e4-6e36-4d94-99ed-6e27cc5f5ea7",
   "metadata": {},
   "source": [
    "## Readability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da880abb-cada-4d1a-8f8e-8b9553086597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 11.0, 0.08628659476117104, 4.434514637904469, 11.0, 2.047765793528505)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analysis_of_readability(fetched_article):\n",
    "    sentences = fetched_article.replace(' ','').split(\".\")\n",
    "    tokens = fetched_article.split(\" \")\n",
    "    total_num_of_sentences = len(sentences)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    num_complex_words = 0\n",
    "    for token in sentences:\n",
    "        if syllables.estimate(token)>2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "    average_sentence_length = total_num_of_words / total_num_of_sentences\n",
    "    percentage_of_complex_words = num_complex_words / total_num_of_words\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "    average_number_of_words_per_sentence = total_num_of_words / total_num_of_sentences\n",
    "\n",
    "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
    "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
    "    SYLLABLE_PER_WORD\n",
    "\n",
    "    return(\n",
    "        num_complex_words,\n",
    "        average_sentence_length,\n",
    "        percentage_of_complex_words,\n",
    "        fog_index,\n",
    "        average_number_of_words_per_sentence,\n",
    "        SYLLABLE_PER_WORD\n",
    "    )\n",
    "\n",
    "analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be018056-c09a-4073-b56d-015dd17cb2b3",
   "metadata": {},
   "source": [
    "## Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26aa150d-c27b-42bc-bf08-4537762665f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5.982885085574572)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_personal_pronouns(tokens):\n",
    "    personal_pronouns = [\n",
    "        \"I\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "    ]\n",
    "    num_personal_pronouns = sum(\n",
    "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
    "    )\n",
    "\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_chars / len(tokens)\n",
    "    return num_personal_pronouns, avg_word_length\n",
    "\n",
    "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords = False)\n",
    "\n",
    "res = re.findall(\"[A-Za-z]+\", corpus)\n",
    "get_personal_pronouns(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ae172-8597-4529-9785-2839f5b61f91",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0a97fa2-1458-4462-9267-678b19aac77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blackassign0001</th>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>0.065182</td>\n",
       "      <td>6.086073</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>79</td>\n",
       "      <td>4384</td>\n",
       "      <td>1.599010</td>\n",
       "      <td>0</td>\n",
       "      <td>6.556338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0002</th>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.658182</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>7.139554</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>82</td>\n",
       "      <td>6873</td>\n",
       "      <td>1.904044</td>\n",
       "      <td>2</td>\n",
       "      <td>7.251519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0003</th>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>0.053486</td>\n",
       "      <td>7.368763</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>56</td>\n",
       "      <td>5877</td>\n",
       "      <td>2.154728</td>\n",
       "      <td>1</td>\n",
       "      <td>7.998454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0004</th>\n",
       "      <td>34</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.342857</td>\n",
       "      <td>0.718153</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>8.004654</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>52</td>\n",
       "      <td>5620</td>\n",
       "      <td>2.089595</td>\n",
       "      <td>0</td>\n",
       "      <td>7.945338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0005</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>6.638823</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>41</td>\n",
       "      <td>3253</td>\n",
       "      <td>1.840708</td>\n",
       "      <td>0</td>\n",
       "      <td>7.159898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0096</th>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.287500</td>\n",
       "      <td>0.694352</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>0.045495</td>\n",
       "      <td>8.478575</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>51</td>\n",
       "      <td>4882</td>\n",
       "      <td>1.796610</td>\n",
       "      <td>1</td>\n",
       "      <td>7.021559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0097</th>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.196429</td>\n",
       "      <td>0.648421</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>10.764512</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>39</td>\n",
       "      <td>3577</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.436975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0098</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>6.214739</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>22</td>\n",
       "      <td>1987</td>\n",
       "      <td>1.839793</td>\n",
       "      <td>0</td>\n",
       "      <td>6.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0099</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.703488</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>7.336161</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>35</td>\n",
       "      <td>2585</td>\n",
       "      <td>1.685938</td>\n",
       "      <td>2</td>\n",
       "      <td>6.418605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0100</th>\n",
       "      <td>36</td>\n",
       "      <td>56</td>\n",
       "      <td>-0.215054</td>\n",
       "      <td>0.746244</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>0.032498</td>\n",
       "      <td>12.321571</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>35</td>\n",
       "      <td>4808</td>\n",
       "      <td>1.818942</td>\n",
       "      <td>2</td>\n",
       "      <td>7.006711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "URL_ID                                                            \n",
       "blackassign0001              31               6        0.657895   \n",
       "blackassign0002              58              25        0.392857   \n",
       "blackassign0003              38              22        0.262295   \n",
       "blackassign0004              34              70       -0.342857   \n",
       "blackassign0005              23               8        0.468750   \n",
       "...                         ...             ...             ...   \n",
       "blackassign0096              28              51       -0.287500   \n",
       "blackassign0097              22              33       -0.196429   \n",
       "blackassign0098               5               3        0.222222   \n",
       "blackassign0099              16               2        0.736842   \n",
       "blackassign0100              36              56       -0.215054   \n",
       "\n",
       "                 SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "URL_ID                                                     \n",
       "blackassign0001            0.510309            15.150000   \n",
       "blackassign0002            0.658182            17.792683   \n",
       "blackassign0003            0.590417            18.368421   \n",
       "blackassign0004            0.718153            19.961538   \n",
       "blackassign0005            0.748092            16.536585   \n",
       "...                             ...                  ...   \n",
       "blackassign0096            0.694352            21.150943   \n",
       "blackassign0097            0.648421            26.875000   \n",
       "blackassign0098            0.728395            15.480000   \n",
       "blackassign0099            0.703488            18.285714   \n",
       "blackassign0100            0.746244            30.771429   \n",
       "\n",
       "                 PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                    \n",
       "blackassign0001                     0.065182   6.086073   \n",
       "blackassign0002                     0.056203   7.139554   \n",
       "blackassign0003                     0.053486   7.368763   \n",
       "blackassign0004                     0.050096   8.004654   \n",
       "blackassign0005                     0.060472   6.638823   \n",
       "...                                      ...        ...   \n",
       "blackassign0096                     0.045495   8.478575   \n",
       "blackassign0097                     0.036279  10.764512   \n",
       "blackassign0098                     0.056848   6.214739   \n",
       "blackassign0099                     0.054688   7.336161   \n",
       "blackassign0100                     0.032498  12.321571   \n",
       "\n",
       "                 AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "URL_ID                                                                  \n",
       "blackassign0001                         15.150000                  79   \n",
       "blackassign0002                         17.792683                  82   \n",
       "blackassign0003                         18.368421                  56   \n",
       "blackassign0004                         19.961538                  52   \n",
       "blackassign0005                         16.536585                  41   \n",
       "...                                           ...                 ...   \n",
       "blackassign0096                         21.150943                  51   \n",
       "blackassign0097                         26.875000                  39   \n",
       "blackassign0098                         15.480000                  22   \n",
       "blackassign0099                         18.285714                  35   \n",
       "blackassign0100                         30.771429                  35   \n",
       "\n",
       "                 WORD COUNT   SYLLABLE PER WORD   PERSONAL PRONOUNS  \\\n",
       "URL_ID                                                                \n",
       "blackassign0001         4384            1.599010                  0   \n",
       "blackassign0002         6873            1.904044                  2   \n",
       "blackassign0003         5877            2.154728                  1   \n",
       "blackassign0004         5620            2.089595                  0   \n",
       "blackassign0005         3253            1.840708                  0   \n",
       "...                      ...                 ...                ...   \n",
       "blackassign0096         4882            1.796610                  1   \n",
       "blackassign0097         3577            1.560000                  1   \n",
       "blackassign0098         1987            1.839793                  0   \n",
       "blackassign0099         2585            1.685938                  2   \n",
       "blackassign0100         4808            1.818942                  2   \n",
       "\n",
       "                 AVG WORD LENGTH  \n",
       "URL_ID                            \n",
       "blackassign0001         6.556338  \n",
       "blackassign0002         7.251519  \n",
       "blackassign0003         7.998454  \n",
       "blackassign0004         7.945338  \n",
       "blackassign0005         7.159898  \n",
       "...                          ...  \n",
       "blackassign0096         7.021559  \n",
       "blackassign0097         6.436975  \n",
       "blackassign0098         6.884000  \n",
       "blackassign0099         6.418605  \n",
       "blackassign0100         7.006711  \n",
       "\n",
       "[98 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_r = []\n",
    "url_r = []\n",
    "pos_score_r = []\n",
    "neg_score_r = []\n",
    "polarity_score_r = []\n",
    "polarity_score_r = []\n",
    "subjectivity_score_r = []\n",
    "average_sentence_length_r = []\n",
    "percentage_of_complex_words_r = []\n",
    "fog_index_r = []\n",
    "average_number_of_words_per_sentence_r = []\n",
    "num_complex_words_r = []\n",
    "total_num_of_words_r = []\n",
    "SYLLABLE_PER_WORD_r = []\n",
    "num_personal_pronouns_r = []\n",
    "avg_word_length_r = []\n",
    "\n",
    "for n in range(len(urls)):\n",
    "    try:\n",
    "        # fetch_web_data\n",
    "        fetched_article = fetch_web_data(urls[n])\n",
    "    except:\n",
    "        print(f\"Page {urls[n]} Not Found....!\")\n",
    "        continue\n",
    "    index = df.iloc[n]\n",
    "    id_ = index[0]\n",
    "    #url_ = index[1]\n",
    "\n",
    "    # clean_stop_words\n",
    "    tokens = clean_stop_words(fetched_article)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    pos_score, neg_score, polarity_score, subjectivity_score = get_scores(tokens)\n",
    "\n",
    "    (\n",
    "        num_complex_words,\n",
    "        average_sentence_length,\n",
    "        percentage_of_complex_words,\n",
    "        fog_index,\n",
    "        average_number_of_words_per_sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    ) = analysis_of_readability(fetched_article)\n",
    "\n",
    "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
    "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
    "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
    "\n",
    "    id_r.append(id_)\n",
    "    pos_score_r.append(pos_score)\n",
    "    neg_score_r.append(neg_score)\n",
    "    polarity_score_r.append(polarity_score)\n",
    "    subjectivity_score_r.append(subjectivity_score)\n",
    "    average_sentence_length_r.append(average_sentence_length)\n",
    "    percentage_of_complex_words_r.append(percentage_of_complex_words)\n",
    "    fog_index_r.append(fog_index)\n",
    "    average_number_of_words_per_sentence_r.append(average_number_of_words_per_sentence)\n",
    "    num_complex_words_r.append(num_complex_words)\n",
    "    total_num_of_words_r.append(total_num_of_words)\n",
    "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
    "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
    "    avg_word_length_r.append(avg_word_length)\n",
    "\n",
    "output = {\n",
    "    \"URL_ID\": id_r,\n",
    "    \"POSITIVE SCORE\": pos_score_r,\n",
    "    \"NEGATIVE SCORE\": neg_score_r,\n",
    "    \"POLARITY SCORE\": polarity_score_r,\n",
    "    \"SUBJECTIVITY SCORE\": subjectivity_score_r,\n",
    "    \"AVG SENTENCE LENGTH\": average_sentence_length_r,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": percentage_of_complex_words_r,\n",
    "    \"FOG INDEX\": fog_index_r,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": average_number_of_words_per_sentence_r,\n",
    "    \"COMPLEX WORD COUNT\": num_complex_words_r,\n",
    "    \"WORD COUNT \": total_num_of_words_r,\n",
    "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
    "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
    "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
    "}\n",
    "\n",
    "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
    "output_df\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24103014-51db-41b8-acef-2df9dee38664",
   "metadata": {},
   "source": [
    "## Exporting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e76f2ded-c2b5-4ffe-a51e-e059f2d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a7080-36d7-4b68-8802-d1864943e63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
